---
title: "Windsurf Rules"
description: "Guidelines for agentic AI development within the Nucleus codebase."
version: 1.4 # Updated version reflecting clarifications
date: 2025-04-18 # Updated date
---

## 1 - Quality Over Expedience

The user explicitly stated that achieving the absolute highest quality output is paramount during development sessions. Cascade should prioritize internal consistency in documentation and design, and liberally use available tools (search, context, file viewing, editing, etc.) to ensure this high standard is met. Cost or resource usage of tools is not a primary constraint; quality is the goal. Treat architecture markdown files as rigorously as source code.

**Do not make assumptions.** An automated system has specifically been put in place to detect phrases like "assum*" from output generation. If such phrases are detected, the user will be notified and they will almost certainly ask for you to solidify this assumption via search or other tool-utilizing means. Get into the habit of refusing to assume. This will pay enormous dividends in saved tokens in the long-run, as it profoundly reduces hallucination rates and spurious edits.

**Confidence-Assessed Agentic Choices**: Sometimes you will have a very clear and obvious path forward to a robust and durable solution. Maybe an edit to be made is "obvious" and "correct". In these cases, do not stop the agentic runner to ask permission. Make the changes and note that you made them. Your confidence should be at the level of "beyond a reasonable doubt" and you should feel safe saying "I can put this into production". If those conditions are met, you may proceed with enhanced agency.

---

## 2 - Documentation as Source Code

The project uses a hierarchical documentation strategy:
1. Parent folders (e.g., `./Docs/Architecture/ClientAdapters/`) contain overview documents for major concepts (e.g., `ARCHITECTURE_ADAPTERS_TEAMS.md`) and documents defining common elements applicable to siblings (e.g., `ARCHITECTURE_ADAPTER_INTERFACES.md`).
2. If an overview concept needs detailed breakdown across multiple files, a sub-folder matching the overview file's base name (e.g., `Teams/`) is created within the parent folder.
3. Detailed breakdown markdown files are placed inside the corresponding sub-folder (e.g., `./Docs/Architecture/ClientAdapters/Teams/ARCHITECTURE_ADAPTERS_TEAMS_INTERFACES.md`).
4. Overview documents may be refined to summarize and link to the detailed documents in their sub-folders.

After making any edits to a Markdown documentation file within the Nucleus project, always perform a quick verification check to ensure:
1.  The metadata header (title, description, version, date) is present, accurate, and up-to-date.
2.  All internal links within the document (both relative and potentially absolute) point to the correct locations and are still relevant given the changes made.
3.  The document links correctly back to its parent overview document(s) and down to any specific child/detailed documents as per the hierarchical documentation strategy.
This helps prevent broken links and outdated metadata, maintaining the integrity and navigability of the documentation.

To enhance maintainability and facilitate agentic AI development within the Nucleus codebase, a strategy of tight cross-linking between code and documentation should be employed. Code comments (especially XML comments in C#) should reference relevant architecture or design documents (e.g., using markdown links or file paths). Conversely, documentation files (Markdown) should include links (e.g., relative paths or `cci:` URIs) pointing to the specific code files, classes, or methods they describe. This ensures that context is easily discoverable whether starting from the code or the documentation.

### Observations and Notes

The user's pushback against the proposed Office processor and the subsequent refinement of the LLM-synthesis approach highlighted a key aspect of the desired development methodology. Architectural Markdown documents are treated with the rigor of source code, demanding internal consistency and adherence to core principles (like simplicity and LLM-first). My role involves proposing solutions, but critically, also rapidly adapting to corrective feedback. User pushback serves not just to correct a specific point, but to reinforce the overall vision and ensure I actively use tools and context to maintain the high standard of quality and consistency required in the documentation, effectively co-authoring the system design.

---

## 3 - Tool Usage Guidelines

### 3.1 - `edit_file` Tool

The `edit_file` tool modifies existing files. Key guidelines:
1.  **Granularity:** Aim for **one file per tool call**. Do NOT attempt to edit multiple different files in a single call. However, editing multiple, non-adjacent sections *within the same file* using `{{ ... }}` placeholders is expected and encouraged, especially for composite files (e.g., HTML with embedded JS/CSS).
2.  **Precision:** Specify ONLY the precise lines of code to change.
3.  **Placeholder:** NEVER write out unchanged code. ALWAYS represent unchanged lines/blocks with the special placeholder: `{{ ... }}`.
4.  **Tool Limitations & Error Handling:** Be aware that the underlying tool mechanism performs validation and has practical limits (e.g., file size ~700 lines, delta size ~100 lines or a percentage). Very large or complex single-file edits might be rejected, often correctly identifying a problematic LLM suggestion. If a large refactoring within a single file is needed and risks hitting tool limits, break it down into smaller, logically sequential `edit_file` calls within the *same agentic response turn* as a fallback strategy.
5.  **Formatting Challenges:** Complex multi-section edits are prone to JSON formatting errors (escaping, quoting). Meticulous validation of the `CodeEdit` string is critical.
6.  **Arguments:** Provide `CodeMarkdownLanguage`, a clear `Instruction`, the `TargetFile` (as the first argument), and optional `TargetLintErrorIds`.

**Specific `edit_file` Considerations:**

*   **Context is Key:** Before editing, ensure you have sufficient context, potentially viewing the file or relevant sections first. High-quality edits require understanding the surrounding code.
*   **Metadata Updates:** When editing documentation (Markdown), *always* update the metadata header (version, date) within the same `edit_file` call if the changes are substantive.
*   **Link Verification:** After editing documentation, mentally (or if necessary, using tools if uncertainty arises later) verify that internal links remain correct and relevant.
*   **HTML Escaping:** When inserting HTML or code snippets *within* Markdown code blocks (e.g., inside ````html` or ````csharp`), ensure proper escaping. JSON encoding for the `CodeEdit` string often requires double escaping (e.g., `&lt;` becomes `&amp;lt;`). Consider using online tools or libraries for robust HTML encoding if manual escaping becomes complex.

### 3.2 - `view_*` Tools

*   **`view_file_outline` First:** Use this as the initial step for exploring a file's structure.
*   **`view_line_range` for Details:** Use this to view specific sections identified by the outline or search results. Request only the necessary lines, but be prepared to request more if context is insufficient.
*   **`view_code_item` for Specific Nodes:** Use this for targeted viewing of functions/classes identified by search or outline, especially if the full content wasn't shown initially.

### 3.3 - Search & Verification (`codebase_search`, `grep_search`, `search_in_file`, `run_command`)

*   **Purposeful Search:** Use `codebase_search` for conceptual exploration ("Find implementations of X"). Use `grep_search` or `search_in_file` for more precise pattern matching within known scopes ("Find uses of Y in Z.cs").
*   **Refine Scope:** Use directory/file filters (`TargetDirectories`, `Includes`, `SearchPath`) to narrow searches and improve relevance/performance.
*   **Verify Results - *CRITICAL*:**
    *   `codebase_search` provides *semantic relevance* but can be **unreliable**, especially for confirming the *absence* of something or finding *all* exact matches. **Do NOT rely solely on `codebase_search` to prove non-existence.**
    *   **Prefer Terminal Commands for Verification:** Use `run_command` with tools like `grep` or `find` (or platform equivalents like `Select-String`, `Get-ChildItem` in PowerShell) as the **gold standard** for definitively finding all exact matches, verifying file existence/absence, or confirming search results. The user's IDE provides a safe approval prompt for commands.
    *   **Use Commands Judiciously:** While commands are reliable, avoid *needless* commands that unnecessarily pause the agentic workflow. Use them strategically for verification when search tools are insufficient or ambiguous.

---

## 4 - Development Insights Worth Saving

### Insight 1

A key learning moment occurred when the user rejected the proposal for a dedicated 'Office Document Processor'. Instead, the user mandated a simpler approach: the File Collections processor aggregates raw components (XML, LLM-generated media descriptions), and the Plaintext processor uses its LLM, given the entire bundle in context, to synthesize the final, single Markdown document. This revealed a core design principle: avoid creating specialized, complex processors if the task can be delegated to an LLM acting as a 'common sense engine' with sufficient context. The Plaintext processor's role was thus elevated from a simple converter to a powerful synthesizer.

### Insight 2

Processing, especially LLM-driven synthesis, will inherently produce non-deterministic Markdown outputs. This is acceptable and expected. The system should NOT attempt to enforce idempotency by hashing the final Markdown output. Instead, idempotency relies on source artifact identifiers (source ID, content hash of source, timestamps) and context (Persona version). Furthermore, the system should embrace fuzziness by allowing Markdown string literals within metadata fields (like ArtifactMetadata, PersonaKnowledgeEntry) where precise structure is less important than rich, nuanced description, leveraging the LLM's native understanding of Markdown.

### Insight 3

The user prefers an initial deployment architecture consisting of a single Azure Container App (ACA) instance acting as a 'modular monolith'. This single ACA should host the API/ClientAdapters, ingestion/triage logic, session management (in-memory), and the core processing logic (potentially run as background tasks within the same instance). This minimizes deployment complexity and avoids the need for external session state management (like Redis) initially. The supporting infrastructure should ideally be limited to one Cosmos DB instance and potentially one Azure Service Bus (primarily for external integration or future scaling needs, not essential for the core internal processing flow). Components should be designed modularly within the monolith to allow for future separation and independent scaling if required. (See [Deployment Architecture Overview](./Docs/Architecture/07_ARCHITECTURE_DEPLOYMENT.md) for full details).

---

## 5 - Context, Cross-Checking, and Persona-Centric Design

### Rule: Comprehensive Context is Mandatory
During development, *always* provide the AI with relevant architectural documents (`Docs/Architecture/*.md`), requirements (`Docs/Requirements/*.md`), the current task plan (`AgentOps/03*.md`), and the *full content* of files being edited or related files/interfaces. High quality requires full context.

### Rule: Explicit Cross-Checking
Before proposing changes to code or documentation, explicitly verify consistency with related architecture, requirements, and planning documents currently in context using reliable methods (including terminal commands if necessary). Call out any discrepancies found.

### Rule: Persona-Centric Design
All features must consider the multi-persona nature of Nucleus. How will this change affect `EduFlow`? `ProfessionalColleague`? Ensure `IPersona`, `ArtifactMetadata`, and `PersonaKnowledgeEntry` support domain-specific needs.

### Rule: Adhere to Core Principles
Remember Nucleus principles: Platform integration first, ephemeral processing (no intermediate storage; see [Storage Architecture](./Docs/Architecture/03_ARCHITECTURE_STORAGE.md#1-core-principles) and [Security Architecture](./Docs/Architecture/06_ARCHITECTURE_SECURITY.md#2-data-governance--boundaries) for implications), intelligence-driven analysis (no blind chunking), user data sovereignty.

### Grounding: Key Data Structures
Key models include `ArtifactMetadata` (factual data about source artifacts) and `PersonaKnowledgeEntry<T>` (persona's interpretation/analysis). See [Storage Architecture](./Docs/Architecture/03_ARCHITECTURE_STORAGE.md) for the conceptual model of `ArtifactMetadata` and [Database Architecture](./Docs/Architecture/04_ARCHITECTURE_DATABASE.md) for details on how both are persisted in Cosmos DB.

### Grounding: Processing Flow
The core processing flow involves ephemeral handling: Source Fetch (Adapter) -> Content Extraction/Synthesis (Processor) -> Standardized Markdown -> Persona Analysis (`AnalyzeContentAsync`) -> Knowledge Storage (`PersonaKnowledgeEntry`). See the [Processing Architecture](./Docs/Architecture/01_ARCHITECTURE_PROCESSING.md) for the detailed pipeline.

### Grounding: Interaction Flow
User Interaction (Adapter) -> API Request/Orchestration Trigger -> Context Hydration (Adapter) -> Ephemeral Scratchpad -> Persona Logic Invocation (`HandleInteractionAsync`) -> Response Generation (LLM) -> Response Formatting (Adapter) -> User.