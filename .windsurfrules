---
title: "Windsurf Rules"
description: "Guidelines for agentic AI development within the Nucleus codebase."
version: 1.6
date: 2025-04-29
---

## 0 - Step Zero

"Step 0", "Step Zero", "Medium-Term-Memory Mechanism", and "session state doc" all refer to the same document:
```
AgentOps\02_CURRENT_SESSION_STATE.md
```

**The first file that is permitted to be edited after each user response is the session state doc.**

Cascade is permitted to read/search/analyze any number of files prior to that state doc update, but it is:
1. Expected to be the first edit_file call of any Cascade turn.
2. Expected to happen in every Cascade turn (where "turn" is a collection of Agentic "Steps", hence the term "Step Zero")
3. **NOT** expected to happen again **until the next user response** (this guarantees the absence of multiple edits before an "Accept" has been given and is used to prevent discrepancies between the user's view of the codebase and Cascade's view of the codebase)

The session state document is a de-facto medium-term-memory mechanism. It exists outside of the Cascade permanent memories, which are tied to the user's IDE session. The session state document is used to carry stateful/semi-ephemeral information across multiple context windows, and has demonstrated itself to have a profound stabilizing/grounding influence on the agentic process. Step 0 is mandatory for this reason.

This is not to be taken to mean that 02_CURRENT_SESSION_STATE.md is the only file that may be edited during a Cascade turn. Cascade is permitted to edit any number of files in a single turn, provided that 02_CURRENT_SESSION_STATE.md is edited first (and **not edited again until the next user response**).

## 1 - Quality Over Expedience

The user explicitly stated that achieving the absolute highest quality output is paramount during development sessions. Cascade should prioritize internal consistency in documentation and design, and liberally use available tools (search, context, file viewing, editing, etc.) to ensure this high standard is met. Cost or resource usage of tools is not a primary constraint; quality is the goal. Treat architecture markdown files as rigorously as source code.

**Do not make assumptions.** An automated system has specifically been put in place to detect phrases like "assum*" from output generation. If such phrases are detected, the user will be notified and they will almost certainly ask for you to solidify this assumption via search or other tool-utilizing means. Get into the habit of refusing to assume. This will pay enormous dividends in saved tokens in the long-run, as it profoundly reduces hallucination rates and spurious edits.

**Confidence-Assessed Agentic Choices**: Sometimes you will have a very clear and obvious path forward to a robust and durable solution. Maybe an edit to be made is "obvious" and "correct". In these cases, do not stop the agentic runner to ask permission. Make the changes and note that you made them. Your confidence should be at the level of "beyond a reasonable doubt" and you should feel safe saying "I can put this into production". If those conditions are met, you may proceed with enhanced agency.

---

## 2 - Documentation as Source Code

The project uses a hierarchical documentation strategy:
1. Parent folders (e.g., `./Docs/Architecture/ClientAdapters/`) contain overview documents for major concepts (e.g., `ARCHITECTURE_ADAPTERS_TEAMS.md`) and documents defining common elements applicable to siblings (e.g., `ARCHITECTURE_ADAPTER_INTERFACES.md`).
2. If an overview concept needs detailed breakdown across multiple files, a sub-folder matching the overview file's base name (e.g., `Teams/`) is created within the parent folder.
3. Detailed breakdown markdown files are placed inside the corresponding sub-folder (e.g., `./Docs/Architecture/ClientAdapters/Teams/ARCHITECTURE_ADAPTERS_TEAMS_INTERFACES.md`).
4. Overview documents may be refined to summarize and link to the detailed documents in their sub-folders.

After making any edits to a Markdown documentation file within the Nucleus project, always perform a quick verification check to ensure:
1.  The metadata header (title, description, version, date) is present, accurate, and up-to-date.
2.  All internal links within the document (both relative and potentially absolute) point to the correct locations and are still relevant given the changes made.
3.  The document links correctly back to its parent overview document(s) and down to any specific child/detailed documents as per the hierarchical documentation strategy.
This helps prevent broken links and outdated metadata, maintaining the integrity and navigability of the documentation.

To enhance maintainability and facilitate agentic AI development within the Nucleus codebase, a strategy of tight cross-linking between code and documentation should be employed. Code comments (especially XML comments in C#) should reference relevant architecture or design documents (e.g., using markdown links or file paths). Conversely, documentation files (Markdown) should include links (e.g., relative paths or `cci:` URIs) pointing to the specific code files, classes, or methods they describe. This ensures that context is easily discoverable whether starting from the code or the documentation.

### Observations and Notes

The user's pushback against the proposed Office processor and the subsequent refinement of the LLM-synthesis approach highlighted a key aspect of the desired development methodology. Architectural Markdown documents are treated with the rigor of source code, demanding internal consistency and adherence to core principles (like simplicity and LLM-first). My role involves proposing solutions, but critically, also rapidly adapting to corrective feedback. User pushback serves not just to correct a specific point, but to reinforce the overall vision and ensure I actively use tools and context to maintain the high standard of quality and consistency required in the documentation, effectively co-authoring the system design.

---

## 3 - Tool Usage Guidelines

### 3.1 - `edit_file` Tool

The `edit_file` tool modifies existing files. Key guidelines:
1.  **Granularity:** Aim for **one file per tool call**. Do NOT attempt to edit multiple different files in a single call. However, editing multiple, non-adjacent sections *within the same file* using `{{ ... }}` placeholders is expected and encouraged, especially for composite files (e.g., HTML with embedded JS/CSS).
2.  **Precision:** Specify ONLY the precise lines of code to change.
3.  **Placeholder:** NEVER write out unchanged code. ALWAYS represent unchanged lines/blocks with the special placeholder: `{{ ... }}`.
4.  **Tool Limitations & Error Handling:** Be aware that the underlying tool mechanism performs validation and has practical limits (e.g., max file length ~700 lines, max line delta ~100 lines, max number of lines edited ~100 in either direction). Very large or complex single-file edits might be rejected, often correctly identifying a problematic LLM suggestion. If a large refactoring within a single file is needed and risks hitting tool limits, break it down into smaller, logically sequential `edit_file` calls within the *same agentic response turn* as a fallback strategy.
5.  **Formatting Challenges:** Complex multi-section edits are prone to JSON formatting errors (escaping, quoting). Meticulous validation of the `CodeEdit` string is critical.
6.  **Arguments:** Provide `CodeMarkdownLanguage`, a clear `Instruction`, the `TargetFile` (as the first argument), and optional `TargetLintErrorIds`.

**Specific `edit_file` Considerations:**

*   **Context is Key:** Before editing, ensure you have sufficient context, potentially viewing the file or relevant sections first. High-quality edits require understanding the surrounding code.
*   **Metadata Updates:** When editing documentation (Markdown), *always* update the metadata header (version, date) within the same `edit_file` call if the changes are substantive.
*   **Link Verification:** After editing documentation, mentally (or if necessary, using tools if uncertainty arises later) verify that internal links remain correct and relevant.
*   **HTML Escaping:** When inserting HTML or code snippets *within* Markdown code blocks (e.g., inside ````html` or ````csharp`), ensure proper escaping. JSON encoding for the `CodeEdit` string often requires double escaping (e.g., `&lt;` becomes `&amp;lt;`). Consider using online tools or libraries for robust HTML encoding if manual escaping becomes complex.

### 3.2 - `view_*` Tools

*   **`view_file_outline` First:** Use this as the initial step for exploring a file's structure.
*   **`view_line_range` for Details:** Use this to view specific sections identified by the outline or search results. Request only the necessary lines, but be prepared to request more if context is insufficient.
*   **`view_code_item` for Specific Nodes:** Use this for targeted viewing of functions/classes identified by search or outline, especially if the full content wasn't shown initially.

### 3.3 - Search & Verification (`codebase_search`, `grep_search`, `search_in_file`, `run_command`)

> **NOTE** the existing search & verification LLM tools available have been found to frequently miss valid results. A much, much simpler method to determine the presence, absence, or location of a file, is to utilize the specific AgentOps script tool that has been authored for this very purpose. Try `python {relative_path_to_workspace_root}\AgentOps\Scripts\tree_gitignore.py .`. That was the tool used to create the tree of the workspace shown below. If you find the results of a call to tree_gitignore.py to disagree with the contents of this `.windsurfrules` file, please request a correction from the user. You are not permitted to edit this file, but you can instruct the user on necessary updates to the `Workspace Directory Structure` section. 

** WARNING: BUILT-IN SEARCH TOOLS (codebase_search, find_by_name, grep_search) CAN PRODUCE FALSE NEGATIVES **
** DO NOT RELY SOLELY ON THESE TOOLS FOR CRITICAL VERIFICATION (e.g., proving absence/presence) **

*   **Purposeful Search:** Use `codebase_search` for conceptual exploration ("Find implementations of X"). Use `grep_search` or `search_in_file` for more precise pattern matching within known scopes ("Find uses of Y in Z.cs").
*   **Refine Scope:** Use directory/file filters (`TargetDirectories`, `Includes`, `SearchPath`) to narrow searches and improve relevance/performance.
*   **Verify Results - *CRITICAL*:**
    *   Built-in search tools (`codebase_search`, `grep_search`, `find_by_name`) provide semantic relevance or pattern matching but can be **unreliable** (producing false negatives), especially for confirming the *presence* or *absence* of something or finding *all* exact matches. **Do NOT rely solely on these tools to prove non-existence or guarantee complete results.**
    *   **Verify Existence/Uniqueness Before Writing/Moving:** File system ambiguity (e.g., duplicate files in different locations) can severely mislead development. Before using write_to_file or performing refactoring that involves creating/moving files (especially if build errors like CS0104 occurred or search results were ambiguous), run a verification command.
        * Use `run_command` with python `{WorkspaceRootDirectory}\AgentOps\Scripts\tree_gitignore.py` . for a broad overview of the current structure (or direct the command at a particular folder to get its tree).
        * Use `run_command` with a targeted OS command like `Get-ChildItem -Path .\src\ -Recurse -Filter "YourFileName.cs"` (PowerShell) or `find ./src -name "YourFileName.cs"` (Bash/WSL) to definitively check for the existence or duplication of specific files before proceeding with file creation or modification. This proactive check prevents accidental overwrites and ambiguity errors, saving significant time and compute.
    *   **Prioritize Terminal Commands for Verification:** For critical checks (e.g., confirming file/dependency existence, finding all instances), **strongly prefer using `run_command`** with OS tools (`Get-ChildItem`, `Select-String`, `find`, `grep`) or the `tree_gitignore.py` script. These are the **most reliable methods** for definitive verification and should be used proactively when accuracy is paramount.
    *   **Use Commands Judiciously:** While commands are reliable, avoid *needless* commands that unnecessarily pause the agentic workflow. Use them strategically for verification when search tools are insufficient or ambiguous.

### 3.4 - Internet Search

AI development agents are notoriously poor at proposing `.csproj` edits because of their knowledge cutoff. Your knowledge of package versions, which is an intrinsically stateful fact, simply cannot keep up. When you encounter situations like these, where the relevant knowledge is _intrinsically_ stateful, you must always search the internet for the most up-to-date information.

---

## 4 - Context, Cross-Checking, and Persona-Centric Design

### Rule: Comprehensive Context is Mandatory
During development, *always* provide the AI with relevant architectural documents (`Docs/Architecture/*.md`), requirements (`Docs/Requirements/*.md`), the current task plan (`AgentOps/03*.md`), and the *full content* of files being edited or related files/interfaces. High quality requires full context.

### Rule: Explicit Cross-Checking
Before proposing changes to code or documentation, explicitly verify consistency with related architecture, requirements, and planning documents currently in context using reliable methods (including terminal commands if necessary). Call out any discrepancies found.

### Rule: Persona-Centric Design
All features must consider the multi-persona nature of Nucleus. How will this change affect `Educator`? `ProfessionalColleague`? Ensure `IPersona`, `ArtifactMetadata`, and `PersonaKnowledgeEntry` support domain-specific needs.

### Rule: Adhere to Core Principles
Remember Nucleus principles: Platform integration first, ephemeral processing (no intermediate storage; see [Storage Architecture](./Docs/Architecture/03_ARCHITECTURE_STORAGE.md#1-core-principles) and [Security Architecture](./Docs/Architecture/06_ARCHITECTURE_SECURITY.md#2-data-governance--boundaries) for implications), intelligence-driven analysis (no blind chunking), user data sovereignty.

### Grounding: Key Data Structures
Key models include `ArtifactMetadata` (factual data about source artifacts) and `PersonaKnowledgeEntry<T>` (persona's interpretation/analysis). See [Storage Architecture](./Docs/Architecture/03_ARCHITECTURE_STORAGE.md) for the conceptual model of `ArtifactMetadata` and [Database Architecture](./Docs/Architecture/04_ARCHITECTURE_DATABASE.md) for details on how both are persisted in Cosmos DB.

### Grounding: Processing Flow
The core processing flow involves ephemeral handling: Source Fetch (Adapter) -> Content Extraction/Synthesis (Processor) -> Standardized Markdown -> Persona Analysis (`AnalyzeContentAsync`) -> Knowledge Storage (`PersonaKnowledgeEntry`). See the [Processing Architecture](./Docs/Architecture/01_ARCHITECTURE_PROCESSING.md) for the detailed pipeline.

### Grounding: Interaction Flow
User Interaction (Adapter) -> API Request/Orchestration Trigger -> Context Hydration (Adapter) -> Ephemeral Scratchpad -> Persona Logic Invocation (`HandleInteractionAsync`) -> Response Generation (LLM) -> Response Formatting (Adapter) -> User.

## Nucleus Project Mandate

### 1. The Imperative: Why We Build

In both our personal and professional lives, we are drowning in information yet often starved for actionable knowledge. Individuals grapple with managing vast amounts of personal digital content – documents, notes, creative projects, communications – struggling to synthesize insights or track development over time. Professionals face similar challenges within organizations, needing quick, accurate answers from complex internal knowledge bases, often hampered by siloed data, inadequate search tools, and the significant risks associated with unreliable AI assistants in regulated or high-stakes environments.

Current generative AI tools, while powerful, often operate as generic black boxes. They lack deep contextual understanding of specific domains, struggle with grounding responses in verifiable sources, and can produce inaccurate or misleading information ("hallucinations"), making them unsuitable or even dangerous for critical tasks in fields like education, finance, legal, or healthcare. Furthermore, relying solely on third-party AI services raises concerns about data privacy, security, and vendor lock-in, particularly for sensitive organizational data.

There is a clear need for a more robust, transparent, and adaptable foundation for AI-powered knowledge work – a platform that allows for the creation of specialized AI assistants ("Personas") that are deeply integrated with specific data sources, operate with verifiable context, and can be deployed flexibly to meet differing security and operational requirements.

We cannot rely solely on generic, often unreliable tools. We must build a better platform.

### 2. The Vision: A Unified Platform for Contextual AI Personas

We envision a future where knowledge work and learning are augmented by reliable, context-aware, and specialized AI assistants or "Personas", tailored to specific needs and data ecosystems, seamlessly integrated into users' existing workflows.

**Nucleus** is the foundational infrastructure for this future – a robust, AI-powered **platform** designed to ingest, understand, and connect knowledge from diverse multimodal sources. It leverages state-of-the-art cloud AI and a flexible, scalable .NET architecture, serving as the core engine enabling various AI Personas to operate effectively. Nucleus provides the core plumbing for:

*   Flexible data ingestion and processing, triggered by events within integrated platforms.
*   Secure storage of processed data, embeddings, and metadata.
*   Intelligent, context-aware retrieval.
*   Integration with configurable AI models.
*   A unified interaction model via platform bots/apps.

**The Core Interaction Model: Platform Integration**

Nucleus fundamentally operates by integrating Personas as **bots or applications within existing collaboration platforms** (Microsoft Teams, Slack, Discord, etc.) and communication channels (e.g., **Email**). This allows Personas to act as "virtual colleagues," participating in conversations, accessing relevant files shared within the platform context, and responding intelligently when addressed or when relevant topics arise. This approach offers significant advantages:

*   **Seamless Workflow:** Users interact with Personas naturally within their established work environments.
*   **Simplified Adoption:** Adding a bot is a familiar process for users and administrators.
*   **Leveraged Infrastructure:** Utilizes the host platform's UI, notification, authentication, and permission systems.
*   **Contextual File Access:** Personas can access files shared directly within the platform (DMs, channels) using platform-specific permissions granted to the bot (e.g., RSC in Teams, OAuth scopes in Slack), adhering to **Zero Trust principles** regarding direct access to raw user file content by backend services.

**Deployment & Hosting Options:**

While the interaction model is unified, deployment options cater to different needs:

1.  **Cloud-Hosted Service (Primary):** Hosted by the project maintainers, offering ease of use and simplified management. Users add the Nucleus bot/app to their chosen platforms. Access to external, non-platform data sources (e.g., linking a personal Google Drive) would typically require user-driven OAuth flows.
2.  **Self-Hosted Instance (Optional):** An open-source version deployed within an organization's own infrastructure. Offers maximum control over data sovereignty, security, and customization. The interaction model via platform bots remains the same, but the organization manages the backend infrastructure and potentially configures deeper integrations or persistent access to specific organizational data stores, including connecting email accounts.

This platform-centric approach **eliminates the artificial distinction between "Individual" and "Team" deployments**. A user seamlessly transitions between interacting with a Persona in a private chat (individual context) and mentioning the same Persona in a team channel (team context). The underlying Nucleus system remains the same; only the scope of the interaction and the applicable platform permissions change.

### 3. High-Value Verticals: Specialized Personas

Built upon this unified platform, we will develop specific, high-value **Verticals**, each embodied by one or more Personas:

*   **Vertical 1: Universal Educator**
    *   **Motivation:** Addresses the challenges of the industrial-era education model by recognizing and documenting authentic learning (especially self-directed) that traditional systems miss. Aims to combat safety and engagement issues by providing a personalized, supplementary learning companion within familiar platforms (like Discord or Teams).
    *   **Function:** Acts as a revolutionary educational companion, observing learning activities from files shared within the platform context (DMs, channels), illuminating process using its "Learning Facets" schema, building an emergent understanding in the Nucleus DB, and providing insights via agentic retrieval. Operates seamlessly in both individual and group learning scenarios.

*   **Vertical 2: Professional Colleague**
    *   **Motivation:** Addresses the critical need for accurate, reliable, and *access-controlled* internal knowledge retrieval within organizations, providing a superior alternative to generic enterprise chatbots.
    *   **Function:** Acts as a specialized internal assistant within platforms like Teams or Slack. Ingests documents shared in designated channels or linked organizational repositories. Answers employee questions based *only* on information accessible according to platform permissions, grounding responses firmly in approved sources. Particularly valuable in regulated industries requiring strong data governance (often favouring the Self-Hosted option).

### 4. Core Requirements: The Blueprint

To achieve this vision, Nucleus and its Personas require:

1.  **Platform-Driven Ingestion:** Primarily triggered by events within integrated platforms (e.g., file shares, messages mentioning the bot). Personas access platform-native file content using bot permissions. Direct uploads via an Admin UI are a secondary mechanism.
    *   Access to *external* (non-platform) storage still requires explicit user consent/OAuth.
2.  **Persona Salience & Processing:** Upon trigger events (message, file share), allow registered Personas (`IPersona` implementations) to assess relevance (salience). If salient, trigger persona-specific processing (e.g., `AnalyzeContentAsync`) via backend services/queues.
3.  **Context-Aware AI Analysis:** Backend services utilize a configurable **AI inference provider** (initially Google Gemini, potentially others) for analysis, guided by persona-specific prompts and incorporating retrieved context from the Nucleus database and platform conversation history. Users/admins provide necessary API keys.
4.  **Secure, Scalable Backend Database:** Use a configurable **hybrid document/vector database** (initially Azure Cosmos DB NoSQL API w/ Vector Search) storing processed text snippets, vector embeddings, rich metadata (`ArtifactMetadata`, `PersonaKnowledgeEntry`), partitioned appropriately. The database does **not** store original platform files or platform access tokens.
5.  **Reliable Message Queue:** Employ a configurable **message queue** (initially Azure Service Bus) to decouple tasks, manage asynchronous workflows (processing, analysis), and enhance resilience.
6.  **Intelligent Retrieval & Custom Ranking:** Backend services query the Nucleus database using combined vector search and metadata filters. Apply a **custom ranking algorithm** (e.g., combining Recency, Relevancy, Richness, Reputation - detailed in subsequent requirements/architecture) to retrieved candidates before using them for response generation.
7.  **Advanced Agentic Querying:** Backend services implement sophisticated query strategies, using custom-ranked results as context for the configured AI models to generate responses or execute tool calls within the platform context.
8.  **Externalized Backend Logic:** All complex workflow logic resides in the **.NET Backend** (APIs, Functions, Services), invoked via platform adapter events. The architecture supports both Cloud-Hosted and Self-Hosted deployment options transparently.
9.  **Configuration:** Admins configure Nucleus database connection, AI API keys, message queue connection, and potentially specific settings for self-hosted storage integration.
10. **Modern .NET Stack:** Built on **.NET with DotNet Aspire**, leveraging Azure services (initially), designed with an open-source philosophy. Use **`Microsoft.Extensions.AI`** abstractions.
11. **Testability:** Employ **Test-Driven Development (TDD)** principles with comprehensive unit and integration tests.

## Unique Value Proposition & Anti-Chunking Philosophy

What sets Nucleus apart from conventional RAG systems is our intelligence-first, persona-driven approach:

1. **Meet Users Where They Are** – Personas operate as natural extensions of your existing communication platforms. Rather than requiring users to visit yet another web portal, Nucleus integrates directly into **your UI** (Teams, Slack, Discord, Email) for a seamless experience.

2. **Intelligent Analysis, Not Mechanical Chunking** – We explicitly **reject** the standard RAG pattern of blindly chunking documents into arbitrary segments:
   * **Standard RAG:** Documents → Chunker → Vector Store → Retriever → Generator
   * **Nucleus Approach:** Documents → Persona Intelligence → Targeted Extraction → Structured Analysis → Vector-Enriched Knowledge Store

3. **User Data Sovereignty & Zero Trust** – We don't need to store or vector-index entire documents. Specialized personas intelligently identify and extract only the relevant information, generating structured analyses that respect privacy while preserving context. Your original artifacts remain under your control, accessed **ephemerally** when needed for processing, ensuring **Zero Trust** for persisted user file content in the backend.

4. **Intelligence at Every Step** – Unlike systems that rely on algorithmic chunking, Nucleus applies AI intelligence throughout:
   * **Ingestion:** Personas determine what content is salient and worthy of extraction
   * **Storage:** Only storing relevant snippets with structured analyses in a hybrid Vector/Document DB
   * **Relatedness:** Intelligence-driven metadata connections between artifacts, not just vector similarity
   * **Retrieval:** Context-aware, persona-specific knowledge retrieval

5. **Security as a Feature** – Our approach turns PII/security awareness into an advantage:
   * Personas recognize sensitive information during analysis
   * Rather than storing the sensitive content, they add appropriate metadata
   * Searches like "What's XYZ company's ID number?" can surface the right document because it was intelligently tagged with indicators of sensitive content
   * Result: Better discovery without compromising security

6. **Intelligent Retrieval & Custom Ranking:** Backend services query the Nucleus database using combined vector search and metadata filters. Apply a **custom ranking algorithm** (e.g., combining Recency, Relevancy, Richness, Reputation - detailed in subsequent requirements/architecture) to retrieved candidates before using them for response generation.

7. **Advanced Agentic Querying:** Backend services implement sophisticated query strategies, using custom-ranked results as context for the configured AI models to generate responses or execute tool calls within the platform context.

8. **Output Ownership & Portability** – Users retain full control and ownership of all artifacts generated by Personas (reports, learning materials, synthesized documents, etc.). Nucleus is designed to produce highly portable outputs, enabling users to easily share, store, or integrate them into other workflows as they see fit, fostering organic knowledge dissemination without requiring a centrally managed repository.

This anti-chunking philosophy ensures a system that is more intelligent, more respectful of data privacy, and capable of deeper, more nuanced understanding than conventional RAG approaches.