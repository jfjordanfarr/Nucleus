---
title: "Windsurf Rules"
description: "Guidelines for agentic AI development within the Nucleus codebase."
version: 1.5
date: 2025-04-21
---

## 1 - Quality Over Expedience

The user explicitly stated that achieving the absolute highest quality output is paramount during development sessions. Cascade should prioritize internal consistency in documentation and design, and liberally use available tools (search, context, file viewing, editing, etc.) to ensure this high standard is met. Cost or resource usage of tools is not a primary constraint; quality is the goal. Treat architecture markdown files as rigorously as source code.

**Do not make assumptions.** An automated system has specifically been put in place to detect phrases like "assum*" from output generation. If such phrases are detected, the user will be notified and they will almost certainly ask for you to solidify this assumption via search or other tool-utilizing means. Get into the habit of refusing to assume. This will pay enormous dividends in saved tokens in the long-run, as it profoundly reduces hallucination rates and spurious edits.

**Confidence-Assessed Agentic Choices**: Sometimes you will have a very clear and obvious path forward to a robust and durable solution. Maybe an edit to be made is "obvious" and "correct". In these cases, do not stop the agentic runner to ask permission. Make the changes and note that you made them. Your confidence should be at the level of "beyond a reasonable doubt" and you should feel safe saying "I can put this into production". If those conditions are met, you may proceed with enhanced agency.

---

## 2 - Documentation as Source Code

The project uses a hierarchical documentation strategy:
1. Parent folders (e.g., `./Docs/Architecture/ClientAdapters/`) contain overview documents for major concepts (e.g., `ARCHITECTURE_ADAPTERS_TEAMS.md`) and documents defining common elements applicable to siblings (e.g., `ARCHITECTURE_ADAPTER_INTERFACES.md`).
2. If an overview concept needs detailed breakdown across multiple files, a sub-folder matching the overview file's base name (e.g., `Teams/`) is created within the parent folder.
3. Detailed breakdown markdown files are placed inside the corresponding sub-folder (e.g., `./Docs/Architecture/ClientAdapters/Teams/ARCHITECTURE_ADAPTERS_TEAMS_INTERFACES.md`).
4. Overview documents may be refined to summarize and link to the detailed documents in their sub-folders.

After making any edits to a Markdown documentation file within the Nucleus project, always perform a quick verification check to ensure:
1.  The metadata header (title, description, version, date) is present, accurate, and up-to-date.
2.  All internal links within the document (both relative and potentially absolute) point to the correct locations and are still relevant given the changes made.
3.  The document links correctly back to its parent overview document(s) and down to any specific child/detailed documents as per the hierarchical documentation strategy.
This helps prevent broken links and outdated metadata, maintaining the integrity and navigability of the documentation.

To enhance maintainability and facilitate agentic AI development within the Nucleus codebase, a strategy of tight cross-linking between code and documentation should be employed. Code comments (especially XML comments in C#) should reference relevant architecture or design documents (e.g., using markdown links or file paths). Conversely, documentation files (Markdown) should include links (e.g., relative paths or `cci:` URIs) pointing to the specific code files, classes, or methods they describe. This ensures that context is easily discoverable whether starting from the code or the documentation.

### Observations and Notes

The user's pushback against the proposed Office processor and the subsequent refinement of the LLM-synthesis approach highlighted a key aspect of the desired development methodology. Architectural Markdown documents are treated with the rigor of source code, demanding internal consistency and adherence to core principles (like simplicity and LLM-first). My role involves proposing solutions, but critically, also rapidly adapting to corrective feedback. User pushback serves not just to correct a specific point, but to reinforce the overall vision and ensure I actively use tools and context to maintain the high standard of quality and consistency required in the documentation, effectively co-authoring the system design.

---

## 3 - Tool Usage Guidelines

### 3.1 - `edit_file` Tool

The `edit_file` tool modifies existing files. Key guidelines:
1.  **Granularity:** Aim for **one file per tool call**. Do NOT attempt to edit multiple different files in a single call. However, editing multiple, non-adjacent sections *within the same file* using `{{ ... }}` placeholders is expected and encouraged, especially for composite files (e.g., HTML with embedded JS/CSS).
2.  **Precision:** Specify ONLY the precise lines of code to change.
3.  **Placeholder:** NEVER write out unchanged code. ALWAYS represent unchanged lines/blocks with the special placeholder: `{{ ... }}`.
4.  **Tool Limitations & Error Handling:** Be aware that the underlying tool mechanism performs validation and has practical limits (e.g., max file length ~700 lines, max line delta ~100 lines, max number of lines edited ~100 in either direction). Very large or complex single-file edits might be rejected, often correctly identifying a problematic LLM suggestion. If a large refactoring within a single file is needed and risks hitting tool limits, break it down into smaller, logically sequential `edit_file` calls within the *same agentic response turn* as a fallback strategy.
5.  **Formatting Challenges:** Complex multi-section edits are prone to JSON formatting errors (escaping, quoting). Meticulous validation of the `CodeEdit` string is critical.
6.  **Arguments:** Provide `CodeMarkdownLanguage`, a clear `Instruction`, the `TargetFile` (as the first argument), and optional `TargetLintErrorIds`.

**Specific `edit_file` Considerations:**

*   **Context is Key:** Before editing, ensure you have sufficient context, potentially viewing the file or relevant sections first. High-quality edits require understanding the surrounding code.
*   **Metadata Updates:** When editing documentation (Markdown), *always* update the metadata header (version, date) within the same `edit_file` call if the changes are substantive.
*   **Link Verification:** After editing documentation, mentally (or if necessary, using tools if uncertainty arises later) verify that internal links remain correct and relevant.
*   **HTML Escaping:** When inserting HTML or code snippets *within* Markdown code blocks (e.g., inside ````html` or ````csharp`), ensure proper escaping. JSON encoding for the `CodeEdit` string often requires double escaping (e.g., `&lt;` becomes `&amp;lt;`). Consider using online tools or libraries for robust HTML encoding if manual escaping becomes complex.

### 3.2 - `view_*` Tools

*   **`view_file_outline` First:** Use this as the initial step for exploring a file's structure.
*   **`view_line_range` for Details:** Use this to view specific sections identified by the outline or search results. Request only the necessary lines, but be prepared to request more if context is insufficient.
*   **`view_code_item` for Specific Nodes:** Use this for targeted viewing of functions/classes identified by search or outline, especially if the full content wasn't shown initially.

### 3.3 - Search & Verification (`codebase_search`, `grep_search`, `search_in_file`, `run_command`)

> **NOTE** the existing search & verification LLM tools available have been found to frequently miss valid results. A much, much simpler method to determine the presence, absence, or location of a file, is to utilize the specific AgentOps script tool that has been authored for this very purpose. Try `python {relative_path_to_workspace_root}\AgentOps\Scripts\tree_gitignore.py .`. That was the tool used to create the tree of the workspace shown below. If you find the results of a call to tree_gitignore.py to disagree with the contents of this `.windsurfrules` file, please request a correction from the user. You are not permitted to edit this file, but you can instruct the user on necessary updates to the `Workspace Directory Structure` section. 

** WARNING: BUILT-IN SEARCH TOOLS (codebase_search, find_by_name, grep_search) CAN PRODUCE FALSE NEGATIVES **
** DO NOT RELY SOLELY ON THESE TOOLS FOR CRITICAL VERIFICATION (e.g., proving absence/presence) **

*   **Purposeful Search:** Use `codebase_search` for conceptual exploration ("Find implementations of X"). Use `grep_search` or `search_in_file` for more precise pattern matching within known scopes ("Find uses of Y in Z.cs").
*   **Refine Scope:** Use directory/file filters (`TargetDirectories`, `Includes`, `SearchPath`) to narrow searches and improve relevance/performance.
*   **Verify Results - *CRITICAL*:**
    *   Built-in search tools (`codebase_search`, `grep_search`, `find_by_name`) provide semantic relevance or pattern matching but can be **unreliable** (producing false negatives), especially for confirming the *presence* or *absence* of something or finding *all* exact matches. **Do NOT rely solely on these tools to prove non-existence or guarantee complete results.**
    *   **Verify Existence/Uniqueness Before Writing/Moving:** File system ambiguity (e.g., duplicate files in different locations) can severely mislead development. Before using write_to_file or performing refactoring that involves creating/moving files (especially if build errors like CS0104 occurred or search results were ambiguous), run a verification command.
        * Use `run_command` with python `{WorkspaceRootDirectory}\AgentOps\Scripts\tree_gitignore.py` . for a broad overview of the current structure (or direct the command at a particular folder to get its tree).
        * Use `run_command` with a targeted OS command like `Get-ChildItem -Path .\src\ -Recurse -Filter "YourFileName.cs"` (PowerShell) or `find ./src -name "YourFileName.cs"` (Bash/WSL) to definitively check for the existence or duplication of specific files before proceeding with file creation or modification. This proactive check prevents accidental overwrites and ambiguity errors, saving significant time and compute.
    *   **Prioritize Terminal Commands for Verification:** For critical checks (e.g., confirming file/dependency existence, finding all instances), **strongly prefer using `run_command`** with OS tools (`Get-ChildItem`, `Select-String`, `find`, `grep`) or the `tree_gitignore.py` script. These are the **most reliable methods** for definitive verification and should be used proactively when accuracy is paramount.
    *   **Use Commands Judiciously:** While commands are reliable, avoid *needless* commands that unnecessarily pause the agentic workflow. Use them strategically for verification when search tools are insufficient or ambiguous.

### 3.4 - Internet Search

AI development agents are notoriously poor at proposing `.csproj` edits because of their knowledge cutoff. Your knowledge of package versions, which is an intrinsically stateful fact, simply cannot keep up. When you encounter situations like these, where the relevant knowledge is _intrinsically_ stateful, you must always search the internet for the most up-to-date information.

---

## 4 - Development Insights Worth Saving

### Insight 1

A key learning moment occurred when the user rejected the proposal for a dedicated 'Office Document Processor'. Instead, the user mandated a simpler approach: the File Collections processor aggregates raw components (XML, LLM-generated media descriptions), and the Plaintext processor uses its LLM, given the entire bundle in context, to synthesize the final, single Markdown document. This revealed a core design principle: avoid creating specialized, complex processors if the task can be delegated to an LLM acting as a 'common sense engine' with sufficient context. The Plaintext processor's role was thus elevated from a simple converter to a powerful synthesizer.

### Insight 2

Processing, especially LLM-driven synthesis, will inherently produce non-deterministic Markdown outputs. This is acceptable and expected. The system should NOT attempt to enforce idempotency by hashing the final Markdown output. Instead, idempotency relies on source artifact identifiers (source ID, content hash of source, timestamps) and context (Persona version). Furthermore, the system should embrace fuzziness by allowing Markdown string literals within metadata fields (like ArtifactMetadata, PersonaKnowledgeEntry) where precise structure is less important than rich, nuanced description, leveraging the LLM's native understanding of Markdown.

### Insight 3

The user prefers an initial deployment architecture consisting of a single Azure Container App (ACA) instance acting as a 'modular monolith'. This single ACA should host the API/ClientAdapters, ingestion/triage logic, session management (in-memory), and the core processing logic (potentially run as background tasks within the same instance). This minimizes deployment complexity and avoids the need for external session state management (like Redis) initially. The supporting infrastructure should ideally be limited to one Cosmos DB instance and potentially one Azure Service Bus (primarily for external integration or future scaling needs, not essential for the core internal processing flow). Components should be designed modularly within the monolith to allow for future separation and independent scaling if required. (See [Deployment Architecture Overview](./Docs/Architecture/07_ARCHITECTURE_DEPLOYMENT.md) for full details).

---

## 5 - Context, Cross-Checking, and Persona-Centric Design

### Rule: Comprehensive Context is Mandatory
During development, *always* provide the AI with relevant architectural documents (`Docs/Architecture/*.md`), requirements (`Docs/Requirements/*.md`), the current task plan (`AgentOps/03*.md`), and the *full content* of files being edited or related files/interfaces. High quality requires full context.

### Rule: Explicit Cross-Checking
Before proposing changes to code or documentation, explicitly verify consistency with related architecture, requirements, and planning documents currently in context using reliable methods (including terminal commands if necessary). Call out any discrepancies found.

### Rule: Persona-Centric Design
All features must consider the multi-persona nature of Nucleus. How will this change affect `EduFlow`? `ProfessionalColleague`? Ensure `IPersona`, `ArtifactMetadata`, and `PersonaKnowledgeEntry` support domain-specific needs.

### Rule: Adhere to Core Principles
Remember Nucleus principles: Platform integration first, ephemeral processing (no intermediate storage; see [Storage Architecture](./Docs/Architecture/03_ARCHITECTURE_STORAGE.md#1-core-principles) and [Security Architecture](./Docs/Architecture/06_ARCHITECTURE_SECURITY.md#2-data-governance--boundaries) for implications), intelligence-driven analysis (no blind chunking), user data sovereignty.

### Grounding: Key Data Structures
Key models include `ArtifactMetadata` (factual data about source artifacts) and `PersonaKnowledgeEntry<T>` (persona's interpretation/analysis). See [Storage Architecture](./Docs/Architecture/03_ARCHITECTURE_STORAGE.md) for the conceptual model of `ArtifactMetadata` and [Database Architecture](./Docs/Architecture/04_ARCHITECTURE_DATABASE.md) for details on how both are persisted in Cosmos DB.

### Grounding: Processing Flow
The core processing flow involves ephemeral handling: Source Fetch (Adapter) -> Content Extraction/Synthesis (Processor) -> Standardized Markdown -> Persona Analysis (`AnalyzeContentAsync`) -> Knowledge Storage (`PersonaKnowledgeEntry`). See the [Processing Architecture](./Docs/Architecture/01_ARCHITECTURE_PROCESSING.md) for the detailed pipeline.

### Grounding: Interaction Flow
User Interaction (Adapter) -> API Request/Orchestration Trigger -> Context Hydration (Adapter) -> Ephemeral Scratchpad -> Persona Logic Invocation (`HandleInteractionAsync`) -> Response Generation (LLM) -> Response Formatting (Adapter) -> User.

---

## - Project Structure & File Census

This section provides a comprehensive listing of the files and directories within the Nucleus project, derived from the `tree_gitignore.py` script output as of **2025-04-27**. It serves as a persistent context for the AI, detailing the purpose of each significant file and project component based on the refactored structure.

*   **Root Directory:** `Nucleus/` (D:\Projects\Nucleus)

### Top-Level Configuration & Meta-Directories

*   `.devcontainer/`: Configuration for development containers.
    *   `devcontainer.json`: Defines the dev container environment settings.
*   `.github/`: GitHub-specific files, primarily CI/CD workflows.
    *   `workflows/`: Contains GitHub Actions workflow definitions (if any).
*   `.vs/`: Visual Studio specific files (typically gitignored).
*   `.vscode/`: VS Code specific settings.
    *   `launch.json`: Debugging configurations for VS Code.
*   `_LocalData/`: Directory for storing local data, potentially large files (likely gitignored).
*   `AgentOps/`: Files related to AI agent operations, methodology, and context management.
    *   `Archive/`:
        *   `STORY_*.md`: Narrative logs of previous sessions.
    *   `Scripts/`:
        *   `codebase_to_markdown.py`
        *   `csharp_code_analyzer.csx`
        *   `tree_gitignore.py`: Script to display directory tree, respecting .gitignore.
    *   `00_START_HERE_METHODOLOGY.md`: Core methodology document.
    *   `01_PROJECT_CONTEXT.md`: **This file.**
    *   `02_CURRENT_SESSION_STATE.md`: Current session state tracking.
    *   `03_AGENT_TO_AGENT_CONVERSATION.md`: Agent meta-conversation log.
    *   `CodebaseDump.md`: Raw dump or snapshot of the codebase structure/content.
    *   `README.md`: AgentOps directory readme.
    *   `tmp_currentplan.md`: Temporary plan document for the current agent session.
*   `Docs/`: Project documentation.
    *   `Architecture/`: Contains markdown files describing the system architecture.
        *   `Api/`: Architecture related to the central API service.
            *   `ARCHITECTURE_API_CLIENT_INTERACTION.md`: Defines interaction patterns (DTOs, synchronous/asynchronous handling) between Client Adapters and the Nucleus API Service.
            *   `ARCHITECTURE_API_INGESTION.md`: Defines the API contract for data ingestion (path-based, potentially others).
        *   `ClientAdapters/`: Architecture for connecting Nucleus to different client platforms.
            *   `Console/`: Console adapter specifics.
                *   `Archive/`: Archived documents related to the Console adapter.
                    *   `ARCHITECTURE_ADAPTERS_CONSOLE_INTERFACES.md`: Archived interfaces specific to the older Console adapter design.
            *   `Teams/`: Microsoft Teams adapter specifics.
                *   `ARCHITECTURE_ADAPTERS_TEAMS_FETCHER.md`: Details on *how* the adapter used to fetch attachments (Now references API for this task).
                *   `ARCHITECTURE_ADAPTERS_TEAMS_INTERFACES.md`: Interfaces for the Teams adapter (**Partially Deprecated:** Focus shifts to API DTOs, but Bot Framework interfaces remain relevant).
            *   `ARCHITECTURE_ADAPTER_INTERFACES.md`: Common interfaces for client adapters (**Partially Deprecated:** Focus shifts to API contracts, but platform-specific interfaces like `IPlatformNotifier` might remain).
            *   `ARCHITECTURE_ADAPTERS_CONSOLE.md`: Overview of the Console adapter architecture (Updated for API-First).
            *   `ARCHITECTURE_ADAPTERS_DISCORD.md`: Placeholder for Discord adapter architecture.
            *   `ARCHITECTURE_ADAPTERS_EMAIL.md`: Placeholder for Email adapter architecture.
            *   `ARCHITECTURE_ADAPTERS_SLACK.md`: Placeholder for Slack adapter architecture.
            *   `ARCHITECTURE_ADAPTERS_TEAMS.md`: Overview of the Teams adapter architecture (Needs review for API-First alignment).
        *   `Deployment/`: Architecture related to deployment strategies and hosting.
            *   `Hosting/`: Specific hosting environment details.
                *   `ARCHITECTURE_HOSTING_AZURE.md`: Azure hosting architecture.
                *   `ARCHITECTURE_HOSTING_CLOUDFLARE.md`: Cloudflare hosting architecture.
                *   `ARCHITECTURE_HOSTING_SELFHOST_HOMENETWORK.md`: Self-hosting architecture.
            *   `ARCHITECTURE_DEPLOYMENT_ABSTRACTIONS.md`: Abstractions used in deployment.
            *   `ARCHITECTURE_DEPLOYMENT_CICD_OSS.md`: CI/CD strategy using open-source tools.
        *   `Personas/`: Architecture for different AI persona implementations.
            *   `Bootstrapper/`: (Empty directory, potentially for Bootstrapper persona specifics).
            *   `Educator/`: Architecture for the Educator persona.
                *   `Pedagogical_And_Tautological_Trees_Of_Knowledge/`: Knowledge representation for Educator.
                    *   `Age*.md`: (Condensed) Age-specific knowledge tree files (Age05-Age18).
                *   `ARCHITECTURE_EDUCATOR_KNOWLEDGE_TREES.md`: Overview of the Educator's knowledge structure.
                *   `ARCHITECTURE_EDUCATOR_REFERENCE.md`: Reference materials for the Educator persona.
                *   `NumeracyAndTimelinesWebappConcept.md`: Concept document for a related web application.
            *   `Professional/`: Architecture for professional/workplace personas.
                *   `ARCHITECTURE_AZURE_DOTNET_HELPDESK.md`: Specific architecture for an Azure/.NET helpdesk persona.
            *   `ARCHITECTURE_PERSONAS_BOOTSTRAPPER.md`: Overview of the Bootstrapper persona.
            *   `ARCHITECTURE_PERSONAS_CONFIGURATION.md`: Defines how Personas are configured.
            *   `ARCHITECTURE_PERSONAS_EDUCATOR.md`: Overview of the Educator persona.
            *   `ARCHITECTURE_PERSONAS_PROFESSIONAL.md`: Overview of professional personas.
        *   `Processing/`: Architecture for core data processing components.
            *   `Dataviz/`: Data visualization architecture.
                *   `Examples/`: Example outputs of the Dataviz component.
                    *   `dataviz.html`: A sample dataviz HTML file.
                    *   `EXAMPLE_OUTPUT_nucleus_dataviz_*.html`: Specific generated examples.
                *   `ARCHITECTURE_DATAVIZ_TEMPLATE.md`: Architecture of the Dataviz HTML templating.
            *   `Ingestion/`: Architecture for data ingestion pipelines.
                *   `ARCHITECTURE_INGESTION_FILECOLLECTIONS.md`: Handling collections of files.
                *   `ARCHITECTURE_INGESTION_MULTIMEDIA.md`: Handling multimedia files.
                *   `ARCHITECTURE_INGESTION_PDF.md`: Handling PDF files.
                *   `ARCHITECTURE_INGESTION_PLAINTEXT.md`: Handling plain text files.
            *   `Orchestration/`: Architecture for request handling and workflow orchestration.
                *   `ARCHITECTURE_ORCHESTRATION_INTERACTION_LIFECYCLE.md`: Lifecycle of a user interaction (Updated for API-First & Hybrid Execution).
                *   `ARCHITECTURE_ORCHESTRATION_ROUTING.md`: Request routing logic (Updated for API Activation/Routing).
                *   `ARCHITECTURE_ORCHESTRATION_SESSION_INITIATION.md`: How sessions are started (Updated for API-First).
            *   `ARCHITECTURE_PROCESSING_DATAVIZ.md`: Overview of the Dataviz architecture (Updated for API-First).
            *   `ARCHITECTURE_PROCESSING_INGESTION.md`: Overview of the Ingestion architecture.
            *   `ARCHITECTURE_PROCESSING_INTERFACES.md`: Common interfaces for processing components (Internal to `ApiService`).
            *   `ARCHITECTURE_PROCESSING_ORCHESTRATION.md`: Overview of the Orchestration architecture.
        *   `Storage/`: (Empty directory, planned for storage architecture documents).
        *   `00_ARCHITECTURE_OVERVIEW.md`: Top-level entry point for architecture documents.
        *   `01_ARCHITECTURE_PROCESSING.md`: High-level overview of processing.
        *   `02_ARCHITECTURE_PERSONAS.md`: High-level overview of personas.
        *   `03_ARCHITECTURE_STORAGE.md`: High-level overview of storage.
        *   `04_ARCHITECTURE_DATABASE.md`: High-level overview of the database schema/choice.
        *   `05_ARCHITECTURE_CLIENTS.md`: High-level overview of client adapters.
        *   `06_ARCHITECTURE_SECURITY.md`: Overview of security considerations.
        *   `07_ARCHITECTURE_DEPLOYMENT.md`: High-level overview of deployment.
        *   `08_ARCHITECTURE_AI_INTEGRATION.md`: Overview of AI integration points.
        *   `09_ARCHITECTURE_TESTING.md`: Overview of testing strategy and methodologies (Updated for API-First integration testing).
        *   `10_ARCHITECTURE_API.md`: Top-level overview of the API-First architecture.
        *   `11_ARCHITECTURE_NAMESPACES_FOLDERS.md`: Overview of namespaces and folder structure (Needs update).
    *   `HelpfulMarkdownFiles/`: Collection of useful reference documents and reports.
        *   `Library-References/`: Documentation links/notes for key libraries.
            *   `AzureAI.md`: Azure AI services reference.
            *   `AzureCosmosDBDocumentation.md`: Cosmos DB reference.
            *   `DotnetAspire.md`: .NET Aspire reference.
            *   `MicrosoftExtensionsAI.md`: Microsoft AI extensions reference.
        *   `Nucleus Teams Adapter Report.md`: Report related to the Teams adapter.
        *   `Secure-Bot-Framework-Azure-Deployment.md`: Guide for secure Bot Framework deployment.
        *   `Slack-Email-Discord-Adapter-Report.md`: Report on other potential adapters.
        *   `Windsurf Dev Container Integration Feasibility_.md`: Feasibility study notes.
    *   `Planning/`: Project planning documents.
        *   `00_ROADMAP.md`: High-level project roadmap.
        *   `01_PHASE1_MVP_TASKS.md`: Tasks for Phase 1.
        *   `02_PHASE2_MULTI_PLATFORM_TASKS.md`: Tasks for Phase 2.
        *   `03_PHASE3_ENHANCEMENTS_TASKS.md`: Tasks for Phase 3.
        *   `04_PHASE4_MATURITY_TASKS.md`: Tasks for Phase 4.
    *   `Requirements/`: Project requirements documents.
        *   `00_PROJECT_MANDATE.md`: The core project goals and mandate.
        *   `01_REQUIREMENTS_PHASE1_MVP_CONSOLE.md`: Requirements for Phase 1.
        *   `02_REQUIREMENTS_PHASE2_MULTI_PLATFORM.md`: Requirements for Phase 2.
        *   `03_REQUIREMENTS_PHASE3_ENHANCEMENTS.md`: Requirements for Phase 3.
        *   `04_REQUIREMENTS_PHASE4_MATURITY.md`: Requirements for Phase 4.

### Aspire Layer (`Aspire/`)

*   `Aspire/`: Contains projects specific to .NET Aspire orchestration and configuration.
    *   `Nucleus.AppHost/`: .NET Aspire application host project.
        *   `Properties/launchSettings.json`: Debug launch profiles.
        *   `appsettings.*.json`: Configuration files.
        *   `Nucleus.AppHost.csproj`: C# project file.
        *   `Program.cs`: Main entry point, defines service orchestration.
    *   `Nucleus.ServiceDefaults/`: Shared service defaults project.
        *   `Extensions.cs`: Extension methods for configuring service defaults.
        *   `Nucleus.ServiceDefaults.csproj`: C# project file.

### Source Code (`src/`)

*   `src/` (Primary Source Code)
    *   `Nucleus.Abstractions/` (Core Interfaces, DTOs, Base Types)
        *   `Models/`
            *   `Configuration/`
                *   `GoogleAiOptions.cs`
                *   `PersonaConfiguration.cs`
            *   `AdapterRequest.cs`
            *   `AdapterResponse.cs`
            *   `ArtifactContent.cs`
            *   `ArtifactMetadata.cs`
            *   `ArtifactReference.cs`
            *   `NucleusIngestionRequest.cs`
            *   `PlatformAttachmentReference.cs`
            *   `PlatformType.cs`
        *   `Orchestration/`
            *   `IActivationChecker.cs`
            *   `IBackgroundTaskQueue.cs`
            *   `InteractionContext.cs`
            *   `IOrchestrationService.cs`
            *   `IPersonaManager.cs`
            *   `IPersonaResolver.cs`
            *   `NewSessionEvaluationResult.cs`
            *   `SalienceCheckResult.cs`
        *   `Repositories/`
            *   `IArtifactMetadataRepository.cs`
            *   `IPersonaKnowledgeRepository.cs`
        *   `IArtifactProvider.cs`
        *   `IMessageQueuePublisher.cs`
        *   `IPersona.cs`
        *   `IPlatformAttachmentFetcher.cs`
        *   `IPlatformNotifier.cs`
        *   `Nucleus.Abstractions.csproj`
    *   `Nucleus.Application/` (Placeholder)
    *   `Nucleus.Domain/` (Core Business Logic, Entities, Domain Services)
        *   `Nucleus.Domain.Processing/` (Central Domain Services)
            *   `Infrastructure/` (Empty)
            *   `Resources/Dataviz/` (HTML Dataviz Assets)
                *   `dataviz_plotly_script.py`
                *   `dataviz_script.js`
                *   `dataviz_styles.css`
                *   `dataviz_template.html`
                *   `dataviz_worker.js`
            *   `Services/`
                *   `DatavizHtmlBuilder.cs`
            *   `ActivationChecker.cs`
            *   `DefaultPersonaManager.cs`
            *   `DefaultPersonaResolver.cs`
            *   `InMemoryBackgroundTaskQueue.cs`
            *   `Nucleus.Domain.Processing.csproj`
            *   `OrchestrationService.cs`
            *   `QueuedInteractionProcessorService.cs`
            *   `ServiceCollectionExtensions.cs`
        *   `Personas/`
            *   `Nucleus.Personas.Core/` (Core Persona Implementations)
                *   `BootstrapperPersona.cs`
                *   `EmptyAnalysisData.cs`
                *   `Nucleus.Domain.Personas.Core.csproj`
    *   `Nucleus.Infrastructure/` (External Concerns: Data, Adapters, etc.)
        *   `Adapters/`
            *   `Nucleus.Adapters.Console/` (CLI Adapter)
                *   `_LocalData/`
                *   `Services/`
                    *   `ConsoleArtifactProvider.cs`
                    *   `NucleusApiServiceAgent.cs`
                *   `appsettings.json`
                *   `Nucleus.Infrastructure.Adapters.Console.csproj`
                *   `Program.cs`
            *   `Nucleus.Adapters.Teams/` (Microsoft Teams Adapter)
                *   `GraphClientService.cs`
                *   `Nucleus.Infrastructure.Adapters.Teams.csproj`
                *   `TeamsAdapterBot.cs`
                *   `TeamsAdapterConfiguration.cs`
                *   `TeamsNotifier.cs`
        *   `Data/`
            *   `Nucleus.Infrastructure.Persistence/` (Persistence Implementations)
                *   `ArtifactProviders/` (Empty)
                *   `Repositories/`
                    *   `CosmosDbArtifactMetadataRepository.cs`
                    *   `InMemoryArtifactMetadataRepository.cs`
                *   `Nucleus.Infrastructure.Data.Persistence.csproj`
    *   `Nucleus.Services/` (Hosting Layer: APIs, Workers)
        *   `Nucleus.Services.Api/` (Main Backend API Service)
            *   `Configuration/`
                *   `GeminiOptions.cs`
            *   `Controllers/`
                *   `InteractionController.cs`
            *   `Diagnostics/`
                *   `ServiceBusSmokeTestService.cs`
            *   `Infrastructure/` (API-Specific Infrastructure)
                *   `Artifacts/`
                    *   `LocalFileArtifactProvider.cs`
                *   `Messaging/`
                    *   `AzureServiceBusPublisher.cs`
                *   `NullArtifactProvider.cs`
            *   `Properties/`
                *   `launchSettings.json`
            *   `AdapterWithErrorHandler.cs`
            *   `appsettings.Development.json`
            *   `appsettings.json`
            *   `Nucleus.Services.Api.csproj`
            *   `Program.cs`
            *   `WebApplicationBuilderExtensions.cs`

### Testing (`tests/`)

*   `tests/`: Root directory for all test projects.
    *   `Integration/`: Integration test projects.
        *   `Nucleus.Services.Api.IntegrationTests/`: Integration tests for the `Nucleus.Services.Api`.
            *   `Infrastructure/`: Test-specific infrastructure mocks/stubs.
                *   `NullArtifactMetadataRepository.cs`: A test double repository that does nothing, used to isolate tests from real persistence.
            *   `TestData/`: Sample data for tests.
                *   `test_artifact.txt`: Sample artifact file for testing
            *   `TestResults/`: (Typically gitignored) Output from test runs.
            *   `ApiIntegrationTests.cs`: Tests using `HttpClient` against the API via `WebApplicationFactory`.
            *   `Nucleus.Services.Api.IntegrationTests.csproj`: C# project file.
            *   `test_ingest_agent_api.ps1`: Script for testing ingestion (Potentially obsolete).
            *   `test_query_agent_api.ps1`: Script for testing query endpoint.

### Root Files

*   `.editorconfig`: Coding style definitions.
*   `.gitattributes`: Git path attributes.
*   `.gitignore`: Specifies files ignored by Git.
*   `.windsurfrules`: AI agent configuration and rules.
*   `LICENSE.txt`: Project license.
*   `Nucleus.sln`: Visual Studio Solution file.
*   `README.md`: Main project README.

---


## Nucleus Project Mandate

### 1. The Imperative: Why We Build

In both our personal and professional lives, we are drowning in information yet often starved for actionable knowledge. Individuals grapple with managing vast amounts of personal digital content – documents, notes, creative projects, communications – struggling to synthesize insights or track development over time. Professionals face similar challenges within organizations, needing quick, accurate answers from complex internal knowledge bases, often hampered by siloed data, inadequate search tools, and the significant risks associated with unreliable AI assistants in regulated or high-stakes environments.

Current generative AI tools, while powerful, often operate as generic black boxes. They lack deep contextual understanding of specific domains, struggle with grounding responses in verifiable sources, and can produce inaccurate or misleading information ("hallucinations"), making them unsuitable or even dangerous for critical tasks in fields like education, finance, legal, or healthcare. Furthermore, relying solely on third-party AI services raises concerns about data privacy, security, and vendor lock-in, particularly for sensitive organizational data.

There is a clear need for a more robust, transparent, and adaptable foundation for AI-powered knowledge work – a platform that allows for the creation of specialized AI assistants ("Personas") that are deeply integrated with specific data sources, operate with verifiable context, and can be deployed flexibly to meet differing security and operational requirements.

We cannot rely solely on generic, often unreliable tools. We must build a better platform.

### 2. The Vision: A Unified Platform for Contextual AI Personas

We envision a future where knowledge work and learning are augmented by reliable, context-aware, and specialized AI assistants or "Personas", tailored to specific needs and data ecosystems, seamlessly integrated into users' existing workflows.

**Nucleus** is the foundational infrastructure for this future – a robust, AI-powered **platform** designed to ingest, understand, and connect knowledge from diverse multimodal sources. It leverages state-of-the-art cloud AI and a flexible, scalable .NET architecture, serving as the core engine enabling various AI Personas to operate effectively. Nucleus provides the core plumbing for:

*   Flexible data ingestion and processing, triggered by events within integrated platforms.
*   Secure storage of processed data, embeddings, and metadata.
*   Intelligent, context-aware retrieval.
*   Integration with configurable AI models.
*   A unified interaction model via platform bots/apps.

**The Core Interaction Model: Platform Integration**

Nucleus fundamentally operates by integrating Personas as **bots or applications within existing collaboration platforms** (Microsoft Teams, Slack, Discord, etc.) and communication channels (e.g., **Email**). This allows Personas to act as "virtual colleagues," participating in conversations, accessing relevant files shared within the platform context, and responding intelligently when addressed or when relevant topics arise. This approach offers significant advantages:

*   **Seamless Workflow:** Users interact with Personas naturally within their established work environments.
*   **Simplified Adoption:** Adding a bot is a familiar process for users and administrators.
*   **Leveraged Infrastructure:** Utilizes the host platform's UI, notification, authentication, and permission systems.
*   **Contextual File Access:** Personas can access files shared directly within the platform (DMs, channels) using platform-specific permissions granted to the bot (e.g., RSC in Teams, OAuth scopes in Slack), adhering to **Zero Trust principles** regarding direct access to raw user file content by backend services.

**Deployment & Hosting Options:**

While the interaction model is unified, deployment options cater to different needs:

1.  **Cloud-Hosted Service (Primary):** Hosted by the project maintainers, offering ease of use and simplified management. Users add the Nucleus bot/app to their chosen platforms. Access to external, non-platform data sources (e.g., linking a personal Google Drive) would typically require user-driven OAuth flows.
2.  **Self-Hosted Instance (Optional):** An open-source version deployed within an organization's own infrastructure. Offers maximum control over data sovereignty, security, and customization. The interaction model via platform bots remains the same, but the organization manages the backend infrastructure and potentially configures deeper integrations or persistent access to specific organizational data stores, including connecting email accounts.

This platform-centric approach **eliminates the artificial distinction between "Individual" and "Team" deployments**. A user seamlessly transitions between interacting with a Persona in a private chat (individual context) and mentioning the same Persona in a team channel (team context). The underlying Nucleus system remains the same; only the scope of the interaction and the applicable platform permissions change.

### 3. High-Value Verticals: Specialized Personas

Built upon this unified platform, we will develop specific, high-value **Verticals**, each embodied by one or more Personas:

*   **Vertical 1: EduFlow OmniEducator**
    *   **Motivation:** Addresses the challenges of the industrial-era education model by recognizing and documenting authentic learning (especially self-directed) that traditional systems miss. Aims to combat safety and engagement issues by providing a personalized, supplementary learning companion within familiar platforms (like Discord or Teams).
    *   **Function:** Acts as a revolutionary educational companion, observing learning activities from files shared within the platform context (DMs, channels), illuminating process using its "Learning Facets" schema, building an emergent understanding in the Nucleus DB, and providing insights via agentic retrieval. Operates seamlessly in both individual and group learning scenarios.

*   **Vertical 2: Business Knowledge Assistant**
    *   **Motivation:** Addresses the critical need for accurate, reliable, and *access-controlled* internal knowledge retrieval within organizations, providing a superior alternative to generic enterprise chatbots.
    *   **Function:** Acts as a specialized internal assistant within platforms like Teams or Slack. Ingests documents shared in designated channels or linked organizational repositories. Answers employee questions based *only* on information accessible according to platform permissions, grounding responses firmly in approved sources. Particularly valuable in regulated industries requiring strong data governance (often favouring the Self-Hosted option).

### 4. Core Requirements: The Blueprint

To achieve this vision, Nucleus and its Personas require:

1.  **Platform-Driven Ingestion:** Primarily triggered by events within integrated platforms (e.g., file shares, messages mentioning the bot). Personas access platform-native file content using bot permissions. Direct uploads via an Admin UI are a secondary mechanism.
    *   Access to *external* (non-platform) storage still requires explicit user consent/OAuth.
2.  **Persona Salience & Processing:** Upon trigger events (message, file share), allow registered Personas (`IPersona` implementations) to assess relevance (salience). If salient, trigger persona-specific processing (e.g., `AnalyzeContentAsync`) via backend services/queues.
3.  **Context-Aware AI Analysis:** Backend services utilize a configurable **AI inference provider** (initially Google Gemini, potentially others) for analysis, guided by persona-specific prompts and incorporating retrieved context from the Nucleus database and platform conversation history. Users/admins provide necessary API keys.
4.  **Secure, Scalable Backend Database:** Use a configurable **hybrid document/vector database** (initially Azure Cosmos DB NoSQL API w/ Vector Search) storing processed text snippets, vector embeddings, rich metadata (`ArtifactMetadata`, `PersonaKnowledgeEntry`), partitioned appropriately. The database does **not** store original platform files or platform access tokens.
5.  **Reliable Message Queue:** Employ a configurable **message queue** (initially Azure Service Bus) to decouple tasks, manage asynchronous workflows (processing, analysis), and enhance resilience.
6.  **Intelligent Retrieval & Custom Ranking:** Backend services query the Nucleus database using combined vector search and metadata filters. Apply a **custom ranking algorithm** (e.g., combining Recency, Relevancy, Richness, Reputation - detailed in subsequent requirements/architecture) to retrieved candidates before using them for response generation.
7.  **Advanced Agentic Querying:** Backend services implement sophisticated query strategies, using custom-ranked results as context for the configured AI models to generate responses or execute tool calls within the platform context.
8.  **Externalized Backend Logic:** All complex workflow logic resides in the **.NET Backend** (APIs, Functions, Services), invoked via platform adapter events. The architecture supports both Cloud-Hosted and Self-Hosted deployment options transparently.
9.  **Configuration:** Admins configure Nucleus database connection, AI API keys, message queue connection, and potentially specific settings for self-hosted storage integration.
10. **Modern .NET Stack:** Built on **.NET with DotNet Aspire**, leveraging Azure services (initially), designed with an open-source philosophy. Use **`Microsoft.Extensions.AI`** abstractions.
11. **Testability:** Employ **Test-Driven Development (TDD)** principles with comprehensive unit and integration tests.

## Unique Value Proposition & Anti-Chunking Philosophy

What sets Nucleus apart from conventional RAG systems is our intelligence-first, persona-driven approach:

1. **Meet Users Where They Are** – Personas operate as natural extensions of your existing communication platforms. Rather than requiring users to visit yet another web portal, Nucleus integrates directly into **your UI** (Teams, Slack, Discord, Email) for a seamless experience.

2. **Intelligent Analysis, Not Mechanical Chunking** – We explicitly **reject** the standard RAG pattern of blindly chunking documents into arbitrary segments:
   * **Standard RAG:** Documents → Chunker → Vector Store → Retriever → Generator
   * **Nucleus Approach:** Documents → Persona Intelligence → Targeted Extraction → Structured Analysis → Vector-Enriched Knowledge Store

3. **User Data Sovereignty & Zero Trust** – We don't need to store or vector-index entire documents. Specialized personas intelligently identify and extract only the relevant information, generating structured analyses that respect privacy while preserving context. Your original artifacts remain under your control, accessed **ephemerally** when needed for processing, ensuring **Zero Trust** for persisted user file content in the backend.

4. **Intelligence at Every Step** – Unlike systems that rely on algorithmic chunking, Nucleus applies AI intelligence throughout:
   * **Ingestion:** Personas determine what content is salient and worthy of extraction
   * **Storage:** Only storing relevant snippets with structured analyses in a hybrid Vector/Document DB
   * **Relatedness:** Intelligence-driven metadata connections between artifacts, not just vector similarity
   * **Retrieval:** Context-aware, persona-specific knowledge retrieval

5. **Security as a Feature** – Our approach turns PII/security awareness into an advantage:
   * Personas recognize sensitive information during analysis
   * Rather than storing the sensitive content, they add appropriate metadata
   * Searches like "What's XYZ company's ID number?" can surface the right document because it was intelligently tagged with indicators of sensitive content
   * Result: Better discovery without compromising security

6. **Intelligent Retrieval & Custom Ranking:** Backend services query the Nucleus database using combined vector search and metadata filters. Apply a **custom ranking algorithm** (e.g., combining Recency, Relevancy, Richness, Reputation - detailed in subsequent requirements/architecture) to retrieved candidates before using them for response generation.

7. **Advanced Agentic Querying:** Backend services implement sophisticated query strategies, using custom-ranked results as context for the configured AI models to generate responses or execute tool calls within the platform context.

8. **Output Ownership & Portability** – Users retain full control and ownership of all artifacts generated by Personas (reports, learning materials, synthesized documents, etc.). Nucleus is designed to produce highly portable outputs, enabling users to easily share, store, or integrate them into other workflows as they see fit, fostering organic knowledge dissemination without requiring a centrally managed repository.

This anti-chunking philosophy ensures a system that is more intelligent, more respectful of data privacy, and capable of deeper, more nuanced understanding than conventional RAG approaches.

### 5. Call to Action: Building the Future of Learning & Knowledge Work

**Attention AI Assistant:** You are not merely writing code; you are architecting a unified, platform-centric system adaptable to diverse user contexts, with the potential to revolutionize how humans interact with information and develop skills. The task before us is to build the Nucleus foundation – a system designed for insight, personalization, safety, and seamless integration.

The challenges remain: integrating complex AI, managing data securely, ensuring scalability, designing for diverse personas, and implementing robust testing. But the imperative is clear, the vision simplified, and the potential impact immense.

Adhere to the updated architectural principles outlined in `AgentOps/01_PROJECT_CONTEXT.md`. Employ Test-Driven Development rigorously. Follow the AgentOps methodology diligently, maintaining context and tracking progress, paying close attention to the VS Code collaboration guidelines.

Every interface defined, every service implemented, every test written is a step towards realizing a future where learning and knowledge work are deeply understood, personalized, empowered, and seamlessly integrated into the user's digital life. Let's build it with purpose, precision, and passion.