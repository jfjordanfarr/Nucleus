---
title: Processing Architecture - Ingestion
description: Describes the orchestrated ingestion process, converting diverse artifact types into canonical ephemeral textual representations using specialized processors for analysis by Personas.
version: 1.2
date: 2025-04-27
parent: ../01_ARCHITECTURE_PROCESSING.md
---

# Processing Architecture: Ingestion

## 1. Overview

The Nucleus ingestion pipeline is responsible for receiving artifacts from client adapters, processing them into a canonical, **ephemeral full textual representation** where feasible, and creating structured `ArtifactMetadata` to track them. This process prioritizes preserving the complete context of the original artifact, preparing it for nuanced, LLM-driven analysis and salient snippet extraction during the retrieval phase.

It strategically employs Large Language Models (LLMs) with extensive context windows (e.g., 1M+ tokens via Google Gemini) *during* ingestion for tasks requiring common-sense reasoning or complex transformations, moving beyond simple text extraction where beneficial.

## 2. Core Principles

*   **Create Full Textual Representations:** Maximize the capture of information from diverse sources into a canonical Markdown format *ephemerally* during processing.
*   **Leverage Large Context LLMs:** Utilize LLMs for common-sense processing, summarization, description generation (for multimedia), and synthesis *during* ingestion within the session scope.
*   **No Premature Chunking:** Generate the full Markdown representation first (ephemerally); chunking occurs later during retrieval based on query context.
*   **No Direct Embedding of Full Content:** Vector embeddings are generated by Personas based on *derived knowledge and salient snippets*, not the entire generated Markdown.
*   **Metadata is King:** Persistently store rich [`ArtifactMetadata`](../../src/Nucleus.Abstractions/Models/ArtifactMetadata.cs) (pointers to source, status) and [`PersonaKnowledgeEntry`](../../src/Nucleus.Abstractions/Repositories/IPersonaKnowledgeRepository.cs) (structured knowledge, snippets, vectors) in Cosmos DB.
*   **Ephemeral Processing Scope:** All intermediate representations (like extracted text, generated Markdown) exist *only* within the memory/ephemeral container storage scope of a single user request/session. Nucleus does not persistently store this generated content, relying instead on fetching fresh source data via Adapters for each session to ensure data freshness and minimize privacy risks.
*   **Processors Produce Ephemeral Outputs for Personas:** Processors like Plaintext and Multimedia generate ephemeral representations (Markdown, descriptions) consumed immediately by Persona processors within the same session.

## 3. High-Level Flow (Per API Request/Session)

1.  **API Request:**
    *   A Client Adapter identifies relevant artifacts for an interaction.
    *   The Adapter constructs an [`AdapterRequest`](../../src/Nucleus.Abstractions/Models/AdapterRequest.cs) containing context (Platform, User ID, etc.) and a list of [`ArtifactReference`](../../src/Nucleus.Abstractions/Models/ArtifactReference.cs) objects pointing to the artifacts in user-controlled storage.
    *   The Adapter sends the `AdapterRequest` to the central API endpoint: `POST /api/v1/interactions`.
    *   **Reference:** [Api/ARCHITECTURE_API_CLIENT_INTERACTION.md](../Api/ARCHITECTURE_API_CLIENT_INTERACTION.md)
2.  **API Service Orchestration:**
    *   The `Nucleus.Services.Api` (specifically, components like the [`OrchestrationService`](../../src/Nucleus.Abstractions/Orchestration/IOrchestrationService.cs)) receives the `AdapterRequest`.
    *   The service handles session management, persona activation/selection based on the request context.
    *   **Reference:** [Processing/ARCHITECTURE_PROCESSING_ORCHESTRATION.md](./ARCHITECTURE_PROCESSING_ORCHESTRATION.md)
3.  **Ephemeral Content Retrieval & Transformation (As Needed):**
    *   If a Persona requires the content of a referenced artifact for its analysis:
        *   The `OrchestrationService` uses the `ArtifactReference` from the `AdapterRequest`.
        *   It resolves the appropriate [`IArtifactProvider`](../../src/Nucleus.Abstractions/IArtifactProvider.cs) implementation (based on `ReferenceType`).
        *   It calls `IArtifactProvider.GetContentAsync(artifactReference)` to ephemerally fetch the content stream directly from the user's source system.
        *   The fetched stream is routed through relevant transformation processors (e.g., FileCollection -> Multimedia -> Plaintext; or PDF Processor -> Plaintext).
        *   Each processor performs its task, generating intermediate outputs (e.g., extracted components, descriptions, synthesized Markdown) that exist **ephemerally** (in memory or temporary local files within the processing scope).
        *   The final **ephemeral Markdown** representation is produced (typically by the Plaintext processor).
    *   **(Optional Background Task):** For very large artifacts or time-consuming transformations that might exceed API gateway timeouts, the `OrchestrationService` *may* dispatch the retrieval and transformation work to an internal background task queue (e.g., Azure Service Bus triggered function, hosted background service). The results would still be ephemeral and associated with the interaction context.
4.  **Persona Analysis:**
    *   The ephemeral Markdown (or other processed representation) is passed to the relevant `Persona` processor(s) operating within the current interaction context.
5.  **Knowledge Extraction & Persistence:**
    *   Personas analyze the ephemeral representation, extract structured data, identify salient snippets, generate vector embeddings *for those snippets/data*, and store/update `PersonaKnowledgeEntry` records in Cosmos DB.
    *   The `ArtifactMetadata` record for the source artifact is updated in Cosmos DB (e.g., `ProcessedByPersonaX = true`).
6.  **Response Handling:**
    *   The `OrchestrationService` coordinates sending any required response back to the user via the appropriate [`IPlatformNotifier`](../../src/Nucleus.Abstractions/IPlatformNotifier.cs).
7.  **Cleanup:**
    *   All ephemeral representations (fetched content streams, intermediate outputs, final Markdown) are discarded at the end of the API request processing scope (or background task completion).
8.  **(Query Phase):**
    *   Subsequent user interactions trigger separate API requests. Retrieval mechanisms for answering queries primarily use the persisted `PersonaKnowledgeEntry` records in Cosmos DB to find relevant snippets/data, potentially re-fetching source content via `IArtifactProvider` if needed for richer context during response synthesis.

## 4. Performance, Cost, and Caching

The strategy of processing full documents relies heavily on the capabilities of the underlying LLM provider:

*   **Large Context Windows (1M+ Tokens):** Essential for providing the LLM with sufficient context (full document text + conversational history) for high-quality extraction and processing.
*   **Low Cost:** Affordable per-token pricing (e.g., Gemini's ~$0.10-$0.20 per million tokens) makes large-context operations financially viable.
*   **Context Caching:** Crucially, leveraging API-level context caching mechanisms allows the cost and latency of processing a large document to be amortized. After the initial processing (which populates the cache), subsequent LLM interactions involving the same document context become significantly faster and cheaper.

This combination enables the high-quality, context-preserving approach central to Nucleus OmniRAG.

## 5. Specific Format Processors

Details for handling specific formats are documented separately:

*   [Plaintext Files](./Ingestion/ARCHITECTURE_INGESTION_PLAINTEXT.md)
*   [PDF Files](./Ingestion/ARCHITECTURE_INGESTION_PDF.md)
*   [Multimedia Files (Image, Audio, Video)](./Ingestion/ARCHITECTURE_INGESTION_MULTIMEDIA.md)
*   [File Collections (e.g., Zip, Docx)](./Ingestion/ARCHITECTURE_INGESTION_FILECOLLECTIONS.md)
