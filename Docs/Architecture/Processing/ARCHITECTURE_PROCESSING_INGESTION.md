---
title: Processing Architecture - Ingestion
description: Describes a baseline approach to data ingestion, which leverages the fact that many data formats are ultimately plain text, a group of zipped plain text files, or multimedia files which can be described using text. 
version: 1.0
date: 2025-04-13
---

# Processing Architecture: Ingestion

## 1. Overview

The Nucleus ingestion pipeline is responsible for receiving artifacts from client adapters, processing them into a canonical, **ephemeral full textual representation** where feasible, and creating structured `ArtifactMetadata` to track them. This process prioritizes preserving the complete context of the original artifact, preparing it for nuanced, LLM-driven analysis and salient snippet extraction during the retrieval phase.

It strategically employs Large Language Models (LLMs) with extensive context windows (e.g., 1M+ tokens via Google Gemini) *during* ingestion for tasks requiring common-sense reasoning or complex transformations, moving beyond simple text extraction where beneficial.

## 2. Core Principles

*   **Create Full Textual Representations:** Maximize the capture of information from diverse sources into a canonical Markdown format *ephemerally* during processing.
*   **Leverage Large Context LLMs:** Utilize LLMs for common-sense processing, summarization, description generation (for multimedia), and synthesis *during* ingestion within the session scope.
*   **No Premature Chunking:** Generate the full Markdown representation first (ephemerally); chunking occurs later during retrieval based on query context.
*   **No Direct Embedding of Full Content:** Vector embeddings are generated by Personas based on *derived knowledge and salient snippets*, not the entire generated Markdown.
*   **Metadata is King:** Persistently store rich `ArtifactMetadata` (pointers to source, status) and `PersonaKnowledgeEntry` (structured knowledge, snippets, vectors) in Cosmos DB.
*   **Ephemeral Processing Scope:** All intermediate representations (like extracted text, generated Markdown) exist *only* within the memory/ephemeral container storage scope of a single user request/session. Nucleus does not persistently store this generated content, relying instead on fetching fresh source data via Adapters for each session to ensure data freshness and minimize privacy risks.
*   **Processors Produce Ephemeral Outputs for Personas:** Processors like Plaintext and Multimedia generate ephemeral representations (Markdown, descriptions) consumed immediately by Persona processors within the same session.

## 3. High-Level Flow (Per User Request/Session)

1.  **Artifact Identification:** Based on user interaction context (e.g., shared file URI, message content), the Platform Adapter identifies relevant source artifact details (Platform, User ID, Conversation ID, Artifact URI, etc.).
2.  **Prepare Ingestion Request:** The Adapter constructs a [`NucleusIngestionRequest`](cci:2://file:///d:/Projects/Nucleus/Nucleus.Abstractions/Models/NucleusIngestionRequest.cs:10:0-49:1) object containing all necessary context and artifact pointers.
3.  **Publish Ingestion Request:** The Adapter uses the [`IMessageQueuePublisher<NucleusIngestionRequest>`](cci:2://file:///d:/Projects/Nucleus/Nucleus.Abstractions/IMessageQueuePublisher.cs:18:0-30:1) to send the request message to the configured Azure Service Bus Queue (e.g., `nucleus-ingestion-requests`).
4.  **Consume Request:** The [`ServiceBusQueueConsumerService`](cci:2://file:///d:/Projects/Nucleus/Nucleus.ApiService/Infrastructure/Messaging/ServiceBusQueueConsumerService.cs:24:0-174:1) (running as a background service in `Nucleus.ApiService`) receives the message from the queue.
5.  **Initiate Orchestration:** The Consumer Service invokes the [`IOrchestrationService.ProcessIngestionRequestAsync`](cci:2://file:///d:/Projects/Nucleus/Nucleus.Abstractions/IOrchestrationService.cs:24:4-25:93) method, passing the deserialized `NucleusIngestionRequest`.
6.  **Orchestration & Processing (Ephemeral):**
    *   The `OrchestrationService` manages the subsequent flow (session handling, persona selection, etc.).
    *   It uses the appropriate `IPlatformAttachmentFetcher` (resolved via DI based on the request's `PlatformType`) to retrieve the *current version* of the source artifact(s) using details from the request.
    *   The artifact is routed through the relevant processors (e.g., FileCollection -> Multimedia -> Plaintext; or PDF Processor -> Plaintext).
    *   Each processor performs its task, generating intermediate outputs (e.g., extracted components, descriptions, synthesized Markdown) that exist **ephemerally** (in memory or temporary local files).
    *   The final **ephemeral Markdown** representation is produced by the Plaintext processor.
7.  **Persona Analysis:** The ephemeral Markdown is passed to the relevant `Persona` processor(s).
8.  **Knowledge Extraction & Persistence:** Personas analyze the Markdown, extract structured data, identify salient snippets, generate vector embeddings *for those snippets/data*, and store/update `PersonaKnowledgeEntry` records in Cosmos DB.
9.  **Update Metadata:** Update the `ArtifactMetadata` record in Cosmos DB for the source artifact, indicating processing status per persona.
10. **(Response Handling):** If the initial interaction requires a response, the `OrchestrationService` uses the appropriate `IPlatformNotifier` to send status updates or results back to the user via the originating platform.
11. **Cleanup:** Ephemeral representations (like the full Markdown) are discarded at the end of the request processing scope.
12. **(Query Phase):** Subsequent retrieval mechanisms query `PersonaKnowledgeEntry` in Cosmos DB for relevant snippets/structured data to synthesize responses.

## 4. Performance, Cost, and Caching

The strategy of processing full documents relies heavily on the capabilities of the underlying LLM provider:

*   **Large Context Windows (1M+ Tokens):** Essential for providing the LLM with sufficient context (full document text + conversational history) for high-quality extraction and processing.
*   **Low Cost:** Affordable per-token pricing (e.g., Gemini's ~$0.10-$0.20 per million tokens) makes large-context operations financially viable.
*   **Context Caching:** Crucially, leveraging API-level context caching mechanisms allows the cost and latency of processing a large document to be amortized. After the initial processing (which populates the cache), subsequent LLM interactions involving the same document context become significantly faster and cheaper.

This combination enables the high-quality, context-preserving approach central to Nucleus OmniRAG.

## 5. Specific Format Processors

Details for handling specific formats are documented separately:

*   [Plaintext Files](./ARCHITECTURE_INGESTION_PLAINTEXT.md)
*   *Others TBD (e.g., PDF, Office, Image, Audio, Code, Zip)*
