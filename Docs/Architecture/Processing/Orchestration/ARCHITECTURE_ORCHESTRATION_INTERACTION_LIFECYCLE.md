---
title: Architecture - Interaction Processing Lifecycle
description: Details the runtime process for handling a single user interaction, from Pub/Sub subscription trigger to response generation, including the use of the ephemeral scratchpad.
version: 1.0
date: 2025-04-13
---

# Nucleus OmniRAG: Interaction Processing Lifecycle

**Version:** 1.0
**Date:** 2025-04-13

## 1. Introduction

This document describes the end-to-end lifecycle of processing a single user interaction within the Nucleus OmniRAG system. This process typically occurs within the containerized compute runtime (e.g., Azure Container Apps, Docker) as outlined in the [Deployment Abstractions](../Deployment/ARCHITECTURE_DEPLOYMENT_ABSTRACTIONS.md#2-asynchronous-messaging-pubsub). It bridges the gap between an external user request arriving via a [Client Adapter](../ClientAdapters/ARCHITECTURE_ADAPTER_INTERFACES.md) and the final response generated by an LLM-powered [Persona](../02_ARCHITECTURE_PERSONAS.md). The core unit of work is represented by an `IPersonaInteractionContext` ([Adapter Interfaces](../ClientAdapters/ARCHITECTURE_ADAPTER_INTERFACES.md#2-ipersonainteractioncontext)).

The key goals of this lifecycle are:
*   **Decoupling:** Separate the initial request intake from the potentially longer-running processing and response generation.
*   **Contextualization:** Gather all necessary information (message content, history, files) required to fulfill the user's request accurately.
*   **Ephemeral Processing:** Perform the work within a temporary context, minimizing persistent state related to a single interaction, aligning with [Security Principles](../06_ARCHITECTURE_SECURITY.md#3-least-privilege--ephemeral-processing).
*   **Efficiency:** Prepare relevant information concisely for the LLM prompt.

## 2. Trigger and Initiation

The interaction lifecycle begins when a processing component (running within the compute runtime) receives a message *identifier* from its configured asynchronous messaging **subscription** (defined in [Deployment Abstractions](../Deployment/ARCHITECTURE_DEPLOYMENT_ABSTRACTIONS.md#2-asynchronous-messaging-pubsub)).

*   **Message Content:** Crucially, the message received typically contains only essential identifiers (e.g., `InteractionId`, `UserId`, source platform message ID, conversation ID) and **not** the potentially sensitive raw message content itself. This aligns with [Security Principles](../06_ARCHITECTURE_SECURITY.md#5-data-minimization).
*   **Adapter Identification:** The processing component uses metadata associated with the message or subscription (e.g., topic name, message properties) to determine which specific `IClientAdapter` implementation is responsible for handling this interaction based on its origin platform (e.g., Teams, Console).

## 3. Context Hydration via Adapter

Using the identifiers received in the message, the orchestrator invokes methods on the corresponding `IClientAdapter` instance to "hydrate" the `IPersonaInteractionContext`. This involves retrieving the actual data needed for processing:

*   **Fetch Message Content:** Calling a method like `adapter.GetMessageContentAsync(platformMessageId, context)` to retrieve the user's actual message text.
*   **Retrieve Conversation History:** Calling `adapter.GetConversationHistoryAsync(conversationId, context)` to get recent relevant messages.
*   **Access Artifacts:** If the user message references files or other artifacts (e.g., via URIs in `SourceArtifactUris` from `IMessageContext`), calling methods like `adapter.GetArtifactStreamAsync(artifactUri, context)` to retrieve their content.

The adapter implementation translates these calls into platform-specific API requests (e.g., Microsoft Graph API calls for Teams, local file system access for the Console Adapter). All operations occur within the security context provided by the adapter/platform.

## 4. Ephemeral Markdown Scratchpad

Once the initial context is successfully gathered, the orchestrator creates a temporary markdown file.

*   **Location:** This file resides in the container's ephemeral local storage (e.g., mapped to `/tmp/` or a similar non-persistent directory). Its filename is typically derived from the unique `IPersonaInteractionContext.InteractionId` (e.g., `/tmp/{InteractionId}.md`).
*   **Purpose:** Acts as a short-term "working memory" or scratchpad exclusively for the current interaction. It aggregates the *salient*, *processed* information required for the LLM prompt, structured for clarity.
*   **Content:** Contains concise, structured text snippets relevant to the request. This might include:
    *   Key phrases or summaries from recent conversation history.
    *   Extracted text chunks or summaries from retrieved documents/artifacts.
    *   Relevant results from internal vector searches against the [Knowledge Base](../04_ARCHITECTURE_DATABASE.md).
    *   **Important:** It holds *processed derivatives*, not the raw, potentially large source data.
*   **Lifecycle & Security:**
    *   The scratchpad is strictly ephemeral, intended to exist only for the duration of the interaction processing (typically 5-60 minutes).
    *   It lives in **non-persistent storage**. If the container instance restarts or crashes, the file is lost, which is the desired behavior for security ([Security Principles](../06_ARCHITECTURE_SECURITY.md#3-least-privilege--ephemeral-processing)).
    *   It **must** be explicitly deleted by the orchestrator upon successful completion or terminal failure of the interaction handling.

## 5. LLM Prompt Assembly & Invocation

The orchestrator constructs the final prompt destined for the AI Service (e.g., Google Gemini, specified in [Hosting Strategies](../Deployment/Hosting/)). This prompt typically includes:

*   The user's direct request (obtained via the adapter).
*   System instructions (defining the Persona's role, capabilities, tone).
*   Relevant context synthesized from the ephemeral markdown scratchpad.

The orchestrator then invokes the AI service API with the assembled prompt.

## 6. Response Delivery & Cleanup

Upon receiving the generated response from the LLM:

1.  **Deliver Response:** The orchestrator calls the appropriate method on the `IClientAdapter` (e.g., `adapter.SendResponseAsync(responseContent, context)`) to deliver the result back to the user on the originating platform.
2.  **Delete Scratchpad:** The orchestrator **explicitly deletes** the temporary markdown file (`/tmp/{InteractionId}.md`) from the ephemeral storage. This cleanup is critical.
3.  **Dispose Context:** The `IPersonaInteractionContext` may be disposed of, releasing any resources held by the adapter or orchestrator related to this specific interaction.

This completes the lifecycle for processing a single user interaction, ensuring context is gathered, processed ephemerally, used for generation, and cleaned up securely. This flow is central to the overall [Processing Architecture](./01_ARCHITECTURE_PROCESSING.md).